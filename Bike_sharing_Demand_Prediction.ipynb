{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "YJ55k-q6phqO",
        "OVtJsKN_phqQ",
        "U2RJ9gkRphqQ",
        "tgIPom80phqQ",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "n3dbpmDWp1ck",
        "ZWILFDl5p1ck",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "BhH2vgX9EjGr",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jagtapdinesh17/Linear-Regression---Bike-Sharing-Demand-Prediction/blob/main/Bike_sharing_Demand_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name -** Dinesh Shivaji Jagtap\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To increase convenience and mobility, bike rentals are now widely available in urban areas. A reliable supply of rental bikes is a big concern because ensuring prompt access to them is essential to cutting down on public wait times. In this regard, the anticipated hourly bicycle count is especially important.\n",
        "This dataset uses previous usage trends, including temperature, time, and other information, to forecast demand for Seoul's bike sharing programme.\n",
        "\n",
        "*   **In the given dataset there are total 8760 records and 14 columns**\n",
        "*   **First step towards project is to load the dataset and import the required liabraries for performing EDA**\n",
        "\n",
        "*   **Checked the first view of dataset using head, tail and sample method**\n",
        "*   **Checked the null and duplicate value and visualized the same using heatmap**\n",
        "\n",
        "*   **Done the data wrangling process on data to make data compatible for machine learning model**\n",
        "*   **Checked the distribution of target variable rented bike count and handled the class imbalence using square root method**\n",
        "\n",
        "*   **Treated the outliers using VIF method for independent variable**\n",
        "*   **Performed EDA of each variables and analyze the trend with target variable**\n",
        "\n",
        "*   **Finally cleaned and scaled data provided to variour models,the metrics were made to evaluate the model**\n",
        "*  **While developing the machine learning model, it is recommended to check multiple models but mainly we are focusing on R2 score and RMSE score**\n",
        "\n",
        "*   **In machine learning, R2 score is a popular evaluation metric used to assess the performance of regression models. It provides an indication of how well the model fits the data, with values ranging from 0 to 1.**\n",
        "*   **The importance of R2 score in machine learning lies in its ability to help determine the accuracy and generalization of the model. A high R2 score indicates that the model is able to explain a large portion of the variation in the data and is likely to generalize well to new data. On the other hand, a low R2 score indicates that the model is not able to capture the relationships between the variables and may not perform well on new data.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   **Currently Rental Bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as ot lessons the waiting time. Evetually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.**\n",
        "\n",
        "\n",
        "2.   **Through a network of stations, bike sharing systems automate membership, rentals, and bike returns. It is possible for people to hire bicycles at one spot and return them at a different or the same location as needed. Bike rentals are facilitated by membership or request, and the operation is managed by an automated retail network spanning the entire city.**\n",
        "\n",
        "3.  **This dataset uses previous usage trends, including temperature, time, and other information, to forecast demand for Seoul's bike sharing programme.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# libraries that are used for analysis and visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "# to import datetime library\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "\n",
        "# libraries used to pre-process \n",
        "from sklearn import preprocessing, linear_model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# libraries used to implement models\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
        "from sklearn.svm import SVR\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# libraries to evaluate performance\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, mean_absolute_error\n",
        "\n",
        "# Library of warnings would assist in ignoring warnings issued\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# to set max column display\n",
        "pd.pandas.set_option('display.max_columns',None)"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "#let's mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the Seol bike data set from the drive\n",
        "df_bike = pd.read_csv('/content/drive/MyDrive/ML-BIKE SHARING DEMAND PREDICTION/SeoulBikeData.csv',encoding ='latin')"
      ],
      "metadata": {
        "id": "Z6DsKqK8BNKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df_bike.head(10)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.tail(10)"
      ],
      "metadata": {
        "id": "nS-DdRKZB5S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.sample(5)"
      ],
      "metadata": {
        "id": "P-Bern-2B895"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df_bike.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'number of rows : {df_bike.shape[0]}  \\nnumber of columns : {df_bike.shape[1]}')"
      ],
      "metadata": {
        "id": "CpwiAS7NCHvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df_bike.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **We have 14 column in dataset , in this 4 are catagorical and others are numerical columns**"
      ],
      "metadata": {
        "id": "G9XL2cyvCokg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate_value = len(df_bike[df_bike.duplicated()])\n",
        "print('The total duplicate values in dataset are {}'.format(duplicate_value))"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No duplicate value found in this dataset**"
      ],
      "metadata": {
        "id": "sbWqz1LgDjWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(df_bike.isna().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.heatmap(df_bike.isna(), cmap = 'crest')"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From the above heatmap it is clear that there is no null value present in dataset, Data is clean and clear**"
      ],
      "metadata": {
        "id": "o-lANDkmGN1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Provided dataset containes 14 columns and 8760 rows\n",
        "2.   Dataset does not have any null or duplicate value\n",
        "\n",
        "3.   10 columns are numerical values and remaining 4 are catagorical values\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df_bike.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df_bike.describe(include = 'all').T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Date :**year-month-day\n",
        "* **Rented Bike count :**Count of bikes rented at each hour\n",
        "* **Hour :**Hour of the day\n",
        "* **Temperature :**Temperature in Celsius\n",
        "* **Humidity :**%\n",
        "* **Windspeed :**m/s\n",
        "* **Visibility :**10m\n",
        "* **Dew point temperature :**Celsius\n",
        "* **Solar radiation :**MJ/m2\n",
        "* **Rainfall :**mm\n",
        "* **Snowfall :**cm\n",
        "* **Seasons :**Winter, Spring, Summer, Autumn\n",
        "* **Holiday :**Holiday/No holiday\n",
        "* **Functional Day :** No Functioning(Non Functional Hours), Functioning(Functional hours)"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df_bike.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df_bike.columns.tolist():\n",
        "  print(\" unique values in\",i,\"is\",df_bike[i].nunique())"
      ],
      "metadata": {
        "id": "nOK9B7FgISl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# 1st need to rename the column to make simple name for accesing the data "
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike=df_bike.rename(columns={'Rented Bike Count':'rented_bike_count',\n",
        "                                'Date':'date',\n",
        "                                'Hour':'hour',\n",
        "                                'Seasons':'seasons',\n",
        "                                'Holiday':'holiday',\n",
        "                                'Temperature(°C)':'temperature',\n",
        "                                'Humidity(%)':'humidity',\n",
        "                                'Wind speed (m/s)':'wind_speed',\n",
        "                                'Visibility (10m)':'visibility',\n",
        "                                'Dew point temperature(°C)':'dew_point_temperature',\n",
        "                                'Solar Radiation (MJ/m2)':'solar_radiation',\n",
        "                                'Rainfall(mm)':'rainfall',\n",
        "                                'Snowfall (cm)':'snowfall',\n",
        "                                'Functioning Day':'functioning_day'})"
      ],
      "metadata": {
        "id": "flp2x-eR6DHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.columns"
      ],
      "metadata": {
        "id": "yicLCkzp6bbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the day, month and year from date column "
      ],
      "metadata": {
        "id": "Y7kXkOpY6jvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.date = pd.to_datetime(df_bike.date)"
      ],
      "metadata": {
        "id": "5mHcpjIJ6rxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike['day'] = df_bike['date'].dt.day\n",
        "df_bike['month'] = df_bike['date'].dt.month\n",
        "df_bike['year'] = df_bike['date'].dt.year\n",
        "df_bike['weekday'] = df_bike['date'].dt.day_name()"
      ],
      "metadata": {
        "id": "zUW0ym8b64I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now drop the date column by using the drop methods"
      ],
      "metadata": {
        "id": "znpnfDg77Yl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.drop('date', axis = 1 , inplace = True)"
      ],
      "metadata": {
        "id": "r-0kgmbS7gei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.hour.unique()"
      ],
      "metadata": {
        "id": "iUfttTCjggvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'Hour' column follows clear sequence for clear EDA purposes we will convert the 'Hour' column into catagorical variables and then we will lable it."
      ],
      "metadata": {
        "id": "klGA_7zogfRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hour_cat(x):\n",
        "   \n",
        "    ''' \n",
        "    'Hour' column follows clear sequence for clear EDA purposes we will convert \n",
        "    the 'Hour' column into catagorical variables and then we will lable it.\n",
        "    '''\n",
        "    \n",
        "    if x>4 and x<=8:\n",
        "        return 'Early Morning'\n",
        "    elif x>8 and x<=12:\n",
        "        return 'Morning'\n",
        "    elif x>12 and x<=16:\n",
        "        return 'Afternoon'\n",
        "    elif x>16 and x<=20:\n",
        "        return 'Evening'\n",
        "    elif x>20 and x<=24:\n",
        "        return 'Night'\n",
        "    elif x<=4:\n",
        "        return 'Late Night'\n",
        "\n",
        "#apply funtion to make new category column\n",
        "df_bike['hour_cat'] = df_bike['hour'].apply(hour_cat)"
      ],
      "metadata": {
        "id": "-Zzr9IOkgXcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.head()"
      ],
      "metadata": {
        "id": "e3Vdv2prheJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Column name has been changed for the simplicity of the data\n",
        "2.   Date column has been bifurgated and date, month, year extracted from the same and date column droped from the main dataset\n",
        "\n",
        "3.   'Hour' column follows clear sequence for clear EDA purposes we will convert the 'Hour' column into catagorical variables and then we will lable it.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 - Target Variable - Rented Bike Count"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "fig, ax = plt.subplots(1,3, figsize = (16,3))\n",
        "\n",
        "# Distribution analysis\n",
        "dist = sns.distplot(df_bike['rented_bike_count'], ax=ax[0])\n",
        "dist.set_title('Distribution of Rented Bike Count', size = 14)\n",
        "\n",
        "# Bi-variate analysis\n",
        "# Rented Bike Count vs Hour Catagory\n",
        "line = sns.pointplot(x='hour_cat',y='rented_bike_count', data=df_bike, ax=ax[1])\n",
        "line.set_title('Rented Bike Count vs Hour Catagory', size = 14)\n",
        "line.set_xticklabels(line.get_xticklabels(), rotation=75)\n",
        "\n",
        "# Bi-variate analysis\n",
        "# Rented Bike Count Vs Weekday\n",
        "point = sns.pointplot(data=df_bike, x='weekday', y='rented_bike_count', ax=ax[2])\n",
        "point.set_title('Rented Bike Count Vs Weekday', size=14)\n",
        "# Set x-ticks rotation to 90 degrees\n",
        "plt.xticks(rotation=75)"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Point plot clearly shows the distribution of the rentend bike along with hour and weekdays\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Distribution of rented bike count is positively skewed.\n",
        "*   Demand for rented bike is increased at evening \n",
        "\n",
        "*   And demand on sunday is very less\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 - **Hour**"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Bi-variate analysis \n",
        "fig,ax = plt.subplots(1,2,figsize=(12,4))\n",
        "\n",
        "# Rented Bike Count Vs Hour\n",
        "bar = sns.barplot(x='hour', y='rented_bike_count',data=df_bike, ax = ax[0])\n",
        "plt.xlabel('Hour')\n",
        "plt.ylabel('Rented Bike Count')\n",
        "bar.set_title('Rented Bike Count Vs Hour')\n",
        "\n",
        "# Multi-variate analysis\n",
        "point = sns.pointplot(data=df_bike, x='hour', y='rented_bike_count', hue='holiday', ax= ax[1])\n",
        "point.set(xlabel='Hour', ylabel = ' Rented Bike Count', title='Average Bike Rented Vs Hour with year')\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Bar plot and line plot clarly shows the distribution of the variables\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Rentend bike demand increses at 8AM and 6PM during business hour\n",
        "2.   Rented bike follows two pattern for holiday and no-holiday\n",
        "\n",
        "3.   For working day demand for bike is high during the business hour\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Yes, gained insights help to manage the count of bike during the specific time of day**\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 - **Temperature**"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize = (10,4))\n",
        "# Rented Bike count vs Temperature\n",
        "line = sns.lineplot(x = 'temperature', y ='rented_bike_count', data = df_bike)\n",
        "line.set_title('Average rented bike count wrt temperature')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Line chart gives the clear understanding of distribution of temperature vs rental bike count\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   It is observed that rented bike count demand increses when temperature is increses but for high temperature demand slightely decreases.\n",
        "*   So we can say people prefere bike in warm temperature, not to cold or not to hot \n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   This insights help us to manage the count of bike with the temperature\n",
        "\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 - **Humidity**"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize = (10,4))\n",
        "\n",
        "sns.lineplot(x='humidity',y='rented_bike_count',data = df_bike)\n",
        "plt.title('Rented Bike Count Vs Humudity')\n",
        "plt.xlabel('Humidity')\n",
        "plt.ylabel('Rented Bike Count')\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Line chart gives the clear understanding of distribution of humidity vs rental bike count"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Most preferred environmenet is 20 to 90 humidity\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 - **Wind Speed**"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "fig,ax = plt.subplots(1,2,figsize = (13,4))\n",
        "\n",
        "# Univariate analysis\n",
        "dist = sns.distplot(df_bike.wind_speed, ax = ax[0])\n",
        "dist.set_title('Distribution of Windspeed')\n",
        "\n",
        "#Bi-variate analysis \n",
        "# Line Plot\n",
        "group_wind_speed = df_bike.groupby(['wind_speed'])['rented_bike_count'].mean().reset_index()\n",
        "line = sns.lineplot(data=group_wind_speed, x='wind_speed', y = 'rented_bike_count', ax= ax[1])\n",
        "line.set(xlabel='Wind Speed', ylabel = ' Rented Bike Count', title='Average Bike Rented Vs Wind Speed')"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Dist plot shows the distribution of the variable \n",
        "*   Line plot clearly shows the relationship between dependent and independent variable\n",
        "\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Distribution of the windspeed is slightely right skeweed, we will treat it later \n",
        "*   We can say people prefers moderate windspeed but at 7 it is very high exceptionaly\n",
        "\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 - **Visibility**"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "fig,ax = plt.subplots(1,2, figsize=(13,4))\n",
        "\n",
        "# Univariate analysis\n",
        "dist = sns.distplot(df_bike.visibility, ax = ax[0])\n",
        "dist.set_title('Distribution Plot of Visibility')\n",
        "\n",
        "# Bi-variate analysis \n",
        "# Rented Bike Count Vs Visibility\n",
        "sns.scatterplot(x='visibility', y='rented_bike_count', data=df_bike, ax = ax[1])\n",
        "plt.xlabel('Visibility')\n",
        "plt.ylabel('Rented Bike Count')\n",
        "plt.title('Scatter Plot of Rented Bike Count Vs Visibility')"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Dist plot shows the distribution of the variable\n",
        "*   Scatter plot shows scattering of the varibales against the rented bike count\n",
        "\n"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Visibility is hightely left skewed\n",
        "*   As visibility increases demand of rented bike also increases\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 - **Dew Point Temperature**"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "fig,ax = plt.subplots(1,3, figsize=(16,4))\n",
        "\n",
        "# Univariate analysis\n",
        "dist = sns.distplot(df_bike.dew_point_temperature, ax = ax[0])\n",
        "dist.set_title('Distribution Plot of Dew Point Temperature')\n",
        "\n",
        "# Bi-variate analysis \n",
        "# Rented Bike Count Vs Dew Point Temperature\n",
        "scatter = sns.scatterplot(x='dew_point_temperature', y='rented_bike_count', data=df_bike, ax = ax[1])\n",
        "plt.xlabel('Dew Point Temperature') \n",
        "plt.ylabel('Rented Bike Count') \n",
        "scatter.set_title('Scatter Plot of Rented Bike Count Vs Dew Point Temperature')\n",
        "\n",
        "# Line Plot\n",
        "group_dew_point_temperature = df_bike.groupby(['dew_point_temperature'])['rented_bike_count'].mean().reset_index()\n",
        "sns.lineplot(data = group_dew_point_temperature, x ='dew_point_temperature', y = 'rented_bike_count', ax= ax[2])\n",
        "plt.xlabel('Dew Point Temperature') \n",
        "plt.ylabel('Rented Bike Count') \n",
        "plt.title('Line Plot of Rented Bike Count Vs Dew Point Temperature')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Scatter plot shows the distribution of variables on different points\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   Distribution of dew point temperature found slightely skewed on negative side\n",
        "*   Line plot followed pattern same as temperature , as dew point temperature increases the rented bike count demand increases but at high dew point temperature the demand is decreased\n",
        "\n"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 - **Solar Radiation**"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "fig,ax = plt.subplots(1,3, figsize=(16,4))\n",
        "\n",
        "# Univariate analysis\n",
        "dist = sns.distplot(df_bike.solar_radiation, ax = ax[0])\n",
        "dist.set_title('Distribution Plot of Solar Radiation')\n",
        "\n",
        "# Bi-variate analysis \n",
        "# Rented Bike Count Vs Solar Radiation\n",
        "scatter = sns.scatterplot(x='solar_radiation', y='rented_bike_count', data=df_bike, ax = ax[1])\n",
        "scatter.set(xlabel='Solar Radiation', ylabel='Rented Bike Count', title='Scatter Plot of Rented Bike Count Vs Solar Radiation')\n",
        "\n",
        "# Line Plot\n",
        "group_solar_radiation = df_bike.groupby(['solar_radiation'])['rented_bike_count'].mean().reset_index()\n",
        "line = sns.lineplot(data = group_solar_radiation, x ='solar_radiation', y = 'rented_bike_count', ax= ax[2])\n",
        "line.set(xlabel='Solar Radiation', ylabel = ' Rented Bike Count', title='Average Bike Rented Vs Solar Radiation')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Scatter plot shows the distribution of variables on different points\n"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Distribution of solar radiation is right skewed \n",
        "*   And as per line and scatter plot it it found that as solar radiation increases demand of rented bike also increases\n",
        "\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - **Rainfall**"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "fig,ax = plt.subplots(1,2, figsize=(12,4))\n",
        "\n",
        "# Univariate analysis\n",
        "dist = sns.distplot(df_bike.rainfall, ax = ax[0])\n",
        "dist.set_title('Distribution Plot of Rainfall')\n",
        "\n",
        "# Bi-variate analysis \n",
        "# Rented Bike Count Vs Rainfall\n",
        "scatter = sns.scatterplot(x='rainfall', y='rented_bike_count', data=df_bike, ax = ax[1])\n",
        "scatter.set(xlabel='Rainfall', ylabel='Rented Bike Count', title='Scatter Plot of Rented Bike Count Vs Rainfall')\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Distribution of rainfall is very much right skewed\n",
        "*   And as per scatter plot it is clear that people do not prefer the bike in rainfall\n",
        "\n"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 - **Snowfall**"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "fig,ax = plt.subplots(1,2, figsize=(12,4))\n",
        "\n",
        "# Univariate analysis\n",
        "dist = sns.distplot(df_bike.snowfall, ax = ax[0])\n",
        "dist.set_title('Distribution Plot of Snowfall')\n",
        "\n",
        "# Bi-variate analysis \n",
        "# Rented Bike Count Vs Snowfall\n",
        "scatter = sns.scatterplot(x='snowfall', y='rented_bike_count', data=df_bike, ax = ax[1])\n",
        "scatter.set(xlabel='Snowfall', ylabel='Rented Bike Count', title='Scatter Plot of Rented Bike Count Vs Snowfall')\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Distribution of snowfall is very much right skewed\n",
        "\n",
        "*   And as per scatter plot it is clear that people do not prefer the bike in snowfall\n",
        "\n"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 - **Seasons**"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "\n",
        "fig,ax = plt.subplots(1,3, figsize=(16,4))\n",
        "\n",
        "# Univariate analysis\n",
        "count = sns.countplot(x='seasons',data=df_bike, ax=ax[0])\n",
        "count.set_title('Count Plot of Seasons')\n",
        "\n",
        "# Bi-variate analysis \n",
        "# Rented Bike Count Vs Seasons\n",
        "bar = sns.barplot(x='seasons', y='rented_bike_count', data=df_bike, ax = ax[1])\n",
        "bar.set(xlabel='Seasons', ylabel='Rented Bike Count', title='Rented Bike Count Vs Seasons')\n",
        "\n",
        "# Multi-variate analysis\n",
        "cat = sns.barplot(x='seasons', y='rented_bike_count', data=df_bike, hue='holiday', ax= ax[2])\n",
        "cat.set(xlabel='Seasons', ylabel = ' Rented Bike Count', title='Average Bike Rented Vs Seasons with holiday status')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Bar chart clearly shows the distribution of two variables \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   The most preferred season for the rented_bike_count is summer and the least preferred is winter which means that people prefer to rent bikes in warm temperatures.\n",
        "\n",
        "*   Demand on working day is high than the holiday\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 - **Functioning Day**"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "fig,ax = plt.subplots(1,3, figsize=(16,4))\n",
        "\n",
        "# Univariate analysis\n",
        "count = sns.countplot(x='functioning_day', data=df_bike, ax=ax[0])\n",
        "count.set_title('Count Plot of Functioning Day')\n",
        "\n",
        "# Bi-variate analysis \n",
        "# Rented Bike Count Vs Functioning Day\n",
        "bar = sns.barplot(x='functioning_day', y='rented_bike_count', data=df_bike,  ax = ax[1])\n",
        "bar.set(xlabel='Functioning Day', ylabel='Rented Bike Count', title='Rented Bike Count Vs Functioning Day')\n",
        "\n",
        "# Multi-variate analysis\n",
        "cat = sns.barplot(x='functioning_day', y='rented_bike_count', hue='hour_cat', data=df_bike,  ax= ax[2])\n",
        "cat.set(xlabel='Functioning Day', ylabel = ' Rented Bike Count', title='Average Bike Rented Vs Functioning Day with session')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   No any bike rented on no functioning day\n",
        "*   At evening session more of the bike are rented \n",
        "\n"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "plt.figure(figsize=(16,6))\n",
        "sns.heatmap(df_bike.corr(), annot=True, cmap='coolwarm')  "
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   To check the co-relation of the variables to each others\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   Dew point temperature is highly co-related with temperature , we will treat it later\n",
        "\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Missing Values/Null Values Count\n",
        "print(df_bike.isnull().sum())"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# counting duplicate values\n",
        "df_bike.duplicated().sum()"
      ],
      "metadata": {
        "id": "O0J4rfIkXq5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   There is no any null value present in dataset\n",
        "*   There is no duplicate value in dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Let's check the summary of the data\n",
        "df_bike.describe().T"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   As per above summary for numerical features, there is significant difference between 75% percentile and maximum value . It is clear that dataset contains skewness and outliers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qUkdz-XWYZPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Splitting Numerical and Catagorical Features**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ng2akgmzcIug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = ['rented_bike_count','temperature','humidity','wind_speed','visibility','dew_point_temperature','solar_radiation',\n",
        "                      'rainfall','snowfall']"
      ],
      "metadata": {
        "id": "UxiH2GbZcVe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "catagorical_features = ['hour', 'seasons', 'holiday', 'functioning_day', 'day', 'month', 'year', 'weekday']"
      ],
      "metadata": {
        "id": "TSUbr-91cih-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our target variable is rented_bike_count we do not want any transformation as of now in numerical features\n",
        "numerical_features.remove('rented_bike_count')\n",
        "\n",
        "# Also rainfall and snowfall are very highly skewed so we will remove this column \n",
        "numerical_features.remove('rainfall')\n",
        "numerical_features.remove('snowfall')"
      ],
      "metadata": {
        "id": "_Xw990jhc9Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features"
      ],
      "metadata": {
        "id": "c_cgys_wd-AM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.1 : Distribution of Numerical Features**"
      ],
      "metadata": {
        "id": "XfjIG6BCeF4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# figsize\n",
        "plt.figure(figsize=(16,8))\n",
        "# title\n",
        "plt.suptitle('Data Distibution of Numerical Features', fontsize=18, fontweight='bold', y=1.02)\n",
        "\n",
        "for i,col in enumerate(numerical_features):\n",
        "  plt.subplot(2, 3, i+1)                       # subplots 2 rows, 3 columns\n",
        "\n",
        "  # dist plots\n",
        "  sns.distplot(df_bike[col])  \n",
        "  # x-axis label\n",
        "  plt.xlabel(col)  \n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "4wsv1qiieNRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   Some of the numerical features are right skewed\n",
        "*   And some of the features are left skewed\n",
        "\n",
        "*   We need to treat this outlier to get the proper result\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aqyt62c8fHG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# figsize\n",
        "plt.figure(figsize=(12,8))\n",
        "# title\n",
        "plt.suptitle('Outlier Analysis of Numerical Features', fontsize=18, fontweight='bold', y=1.02)\n",
        "\n",
        "for i,col in enumerate(numerical_features):\n",
        "  plt.subplot(2, 3, i+1)            # subplot of 2 rows and 3 columns\n",
        "\n",
        "  # countplot\n",
        "  sns.boxplot(df_bike[col])\n",
        "  # x-axis label\n",
        "  plt.xlabel(col)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "xW0a1_fYf64Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   We found that outlier clearly visible in solar radiation  and windspeed\n",
        "*   We will treat them by using clipping method because we have limited data point in datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "FVv9pCb7hAmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will replace the datapoint with upper and lower limit \n",
        "\n",
        "def cliping_outliers(df_bike):\n",
        "    for col in df_bike[numerical_features]:\n",
        "        # using IQR method to define range of upper and lower limit.\n",
        "        q1 = df_bike[col].quantile(0.25)\n",
        "        q3 = df_bike[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "        \n",
        "        # replacing the outliers with upper and lower bound\n",
        "        df_bike[col] = df_bike[col].clip(lower_bound, upper_bound)\n",
        "    return df_bike"
      ],
      "metadata": {
        "id": "_hz-nMDBidzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the function to treat outliers\n",
        "df_bike = cliping_outliers(df_bike)"
      ],
      "metadata": {
        "id": "Nnr1nxYui7eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the boxplot after treating outliers\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "plt.suptitle('Outlier Analysis of Numerical Features', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "for i,col in enumerate(numerical_features):\n",
        "  plt.subplot(2, 3, i+1)            # subplot of 2 rows and 3 columns\n",
        "\n",
        "  # countplot\n",
        "  sns.boxplot(df_bike[col])\n",
        "  # x-axis label\n",
        "  plt.xlabel(col)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "a_YQCoIWjEx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution after treating outliers.\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "plt.suptitle('Data Distibution of Numerical Features', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "for i,col in enumerate(numerical_features):\n",
        "  plt.subplot(2, 3, i+1)                       # subplots 2 rows, 3 columns\n",
        "\n",
        "  # dist plots\n",
        "  sns.distplot(df_bike[col])  \n",
        "  # x-axis label\n",
        "  plt.xlabel(col)\n",
        "  plt.tight_layout() "
      ],
      "metadata": {
        "id": "q1bdcZSUjlp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   We have treated the numerical features by using the clipping methods.\n",
        "*   We have checked the distribution of the numerical features and found that all features has been normalized\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2-GobIs_j5Ak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clipping Method: In this method, we set a cap on our outliers data, which means that if a value is higher than or lower than a certain threshold, all values will be considered outliers. This method replaces values that fall outside of a specified range with either the minimum or maximum value within that range.**"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.2: Distribution of Target Variable - Rented Bike Count**"
      ],
      "metadata": {
        "id": "b4xk0ZqDknWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2 , figsize = (15,5))\n",
        "\n",
        "# Distribution plot of Rented Bike Count\n",
        "dist =sns.distplot(df_bike['rented_bike_count'], hist=True, ax = ax[0])\n",
        "dist.set(xlabel = 'Rented Bike Count', ylabel ='Density', title = 'Distribution Plot of Target Variable')\n",
        "\n",
        "# mean line\n",
        "dist.axvline(df_bike['rented_bike_count'].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "# median line\n",
        "dist.axvline(df_bike['rented_bike_count'].median(), color='black', linestyle='dashed', linewidth=2)\n",
        "\n",
        "# Boxplot\n",
        "box = sns.boxplot(df_bike.rented_bike_count, ax= ax[1])\n",
        "box.set(title = 'Outlier Analysis of Target Variable')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NOWPsagFk1ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Distribution plot clearly shows that rented bike count is slightely skewed in right side, we will require normal distribution of the same we will treat it using different methods. \n",
        "*   Box plot shows some outliers are present in rented bike count \n",
        "\n"
      ],
      "metadata": {
        "id": "w9qQeW2Rmi70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the best transformation method for our target variables\n",
        "\n",
        "f, axes = plt.subplots(1, 3,figsize=(16,6))\n",
        "sns.distplot(x=np.sqrt(df_bike['rented_bike_count']),color='g',ax=axes[0])\n",
        "sns.distplot(x=np.cbrt(df_bike['rented_bike_count']), color = 'g', ax = axes[1])\n",
        "sns.distplot(x=np.log1p(df_bike['rented_bike_count']), color = 'g', ax = axes[2])\n"
      ],
      "metadata": {
        "id": "AbqkdGjrmeTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   We can see that by using sqrt transformation target variable follows proper pattern of normal distribution\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ppW1qdPyqd6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will consider sqrt method , lets check using the box plot\n",
        "fig, ax = plt.subplots(1,2 , figsize = (12,4))\n",
        "\n",
        "#  checking square root tranformation in our target variable\n",
        "dist =sns.distplot(np.sqrt(df_bike['rented_bike_count']), ax = ax[0])\n",
        "dist.set(xlabel = 'Rented Bike Count', ylabel ='Density', title = 'Distribution Plot of Target Variable in sqrt tranformation')\n",
        "\n",
        "# mean line\n",
        "dist.axvline(np.sqrt(df_bike['rented_bike_count']).mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "# median line\n",
        "dist.axvline(np.sqrt(df_bike['rented_bike_count']).median(), color='black', linestyle='dashed', linewidth=2)\n",
        "\n",
        "# Boxplot\n",
        "box = sns.boxplot(np.sqrt(df_bike.rented_bike_count), ax= ax[1])\n",
        "box.set(title = 'Outlier Analysis of Target Variable in sqrt tranformation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O9D1tbE5qrTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   We found that there is no any outlier present in target variable after the sqrt transformation "
      ],
      "metadata": {
        "id": "6jSZKtqarJY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Checking linearity of numerical features with target variables\n",
        "\n",
        "# figsize\n",
        "plt.figure(figsize=(15,5))\n",
        "# title\n",
        "plt.suptitle('Regression Analysis of Numerical features', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "for i,col in enumerate(numerical_features):\n",
        "  plt.subplot(2, 3, i+1)                     # subplots of 2 rows and 3 columns\n",
        "\n",
        "  # regression plots\n",
        "  sns.regplot(x=df_bike[col], y='rented_bike_count', data=df_bike, line_kws={\"color\":\"r\"})\n",
        "  # x-axis lable\n",
        "  plt.xlabel(col)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Most of the variables are positively co-related with target variables\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gVx-nopfBfNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "plt.figure(figsize=(16,6))\n",
        "sns.heatmap(df_bike.corr(), annot=True, cmap='coolwarm')  "
      ],
      "metadata": {
        "id": "7o2jrHDyPHE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Dew point temperature is highly co-related with temperature and dew point temperature is less co-relation with target variable hence we will drop this column \n",
        "\n"
      ],
      "metadata": {
        "id": "zzkoIrfsPN8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping the column \n",
        "df_bike.drop('dew_point_temperature', axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "kWfDiEQkPJzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation\n",
        "\n",
        "*   Feature engineering is the process of creating new features from existing ones to improve the performance of a machine-learning model. This involves transforming raw data into a more useful and informative form, by either creating new features from the existing data or selecting only the most relevant features from the raw data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VIF - \n",
        "(Variance Inflation Factor) analysis is a statistical method used to identify multicollinearity in a set of predictor variables. Multicollinearity is a situation where two or more predictor variables in a regression model are highly correlated with each other, meaning that they provide redundant information about the response variable."
      ],
      "metadata": {
        "id": "pwuvqn68Cs0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def calc_vif(x):\n",
        "\n",
        "  # For each x, calculate VIF and save in dataframe\n",
        "  vif = pd.DataFrame()\n",
        "  vif[\"VIF Factor\"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]\n",
        "  vif[\"features\"] = x.columns\n",
        "  \n",
        "  return vif"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's calculate VIF \n",
        "calc_vif(df_bike[[i for i in df_bike.describe().columns]])"
      ],
      "metadata": {
        "id": "Tfri4Gt_DDOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VIF for year is too large , so we wil drop the column from the dataset\n",
        "df_bike.drop('year', axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "kql3OFK2QBR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's calculate VIF \n",
        "calc_vif(df_bike[[i for i in df_bike.describe().columns]])"
      ],
      "metadata": {
        "id": "4GgGW8UNQak2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   This are all the total final numerical variables we will consider for model building\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JStrCUpzQhM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection "
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   We have usied VIF method for selection of numerical features for model building\n",
        "\n"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding\n",
        "Encoding is a technique in feature engineering that is used to convert categorical variables into numerical values that can be used by machine learning algorithms."
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "catagorical_features"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing year and weekday from this list which have been removed from dataframe\n",
        "catagorical_features.remove('year')\n",
        "catagorical_features.remove('weekday')"
      ],
      "metadata": {
        "id": "fnHG0KHw4DK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each categorical variable.\n",
        "for i in catagorical_features:\n",
        "  print(\"No. of unique values in\",i,\"is\",df_bike[i].nunique())"
      ],
      "metadata": {
        "id": "XFMT0cEF4pQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical Encoding for holiday and functioning_day\n",
        "\n",
        "df_bike['holiday'] = df_bike['holiday'].map({'Holiday': 1, 'No Holiday': 0})\n",
        "df_bike['functioning_day'] = df_bike['functioning_day'].map({'Yes': 1, 'No': 0})"
      ],
      "metadata": {
        "id": "z57kNrBf4-z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One Hot Encoding\n",
        "\n",
        "# One-hot encode the 'seasons' feature\n",
        "onehot_hourcat = pd.get_dummies(df_bike['seasons'], prefix='seasons')\n",
        "\n",
        "#Droping the original features 'seasons'\n",
        "df_bike.drop('seasons', axis = 1, inplace = True)\n",
        "\n",
        "# concatenate the one-hot encoded season feature with the rest of the data\n",
        "df_bike = pd.concat([df_bike, onehot_hourcat], axis=1)"
      ],
      "metadata": {
        "id": "RDrR9Iik5Jnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.columns"
      ],
      "metadata": {
        "id": "tNq1eYOf8O4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will drop weekday and Hour_Cat because we have taken this column only for EDA purpose"
      ],
      "metadata": {
        "id": "psWuo-gxRBMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.drop('weekday', axis = 1, inplace = True)\n",
        "df_bike.drop('hour_cat', axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "XylrFUmkRJqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.columns"
      ],
      "metadata": {
        "id": "QxYnVOGFRUls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   One-hot encoding: creates a binary column for each unique category, with a value of 1 indicating the presence of the category and 0 indicating the absence.\n",
        "*   Numerical encoding used for holiday and functioning day \n",
        "\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = df_bike.drop('rented_bike_count', axis=1)\n",
        "y= np.sqrt(df_bike['rented_bike_count'])  \n",
        "\n",
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        " \n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.22, random_state=2)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        "*   The StandardScaler method scales the features such that they have zero mean and unit variance. It works by subtracting the mean of each feature from its values and then dividing the result by its standard deviation. This ensures that each feature has a mean of zero and a standard deviation of one. The scaling is done independently for each feature, which means that the scaling of one feature does not depend on the scaling of another feature.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1: Model Training"
      ],
      "metadata": {
        "id": "A38bLaSII1sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Make 1st emply list to append the result at last\n",
        "result = []\n",
        "\n",
        "def predict(ml_model,model_name):\n",
        "  '''\n",
        "  We will pass the model and predict the value.\n",
        "  Function will calculate all the result for evaluation metrics and append to the result list.\n",
        "  Plotting Different graphs for test data\n",
        "  '''\n",
        "\n",
        "  # Fit the model\n",
        "  model = ml_model.fit(x_train,y_train)\n",
        "\n",
        "  #Predicting values\n",
        "  y_train_pred = model.predict(x_train)\n",
        "  y_test_pred = model.predict(x_test)\n",
        "\n",
        "  # Reverse the transformation on the predictions (If we need y_train_pred in original and transformed way)\n",
        "  y_train_pred_original = np.power(y_train_pred, 2)\n",
        "  y_test_pred_original = np.power(y_test_pred, 2)\n",
        "\n",
        "  # Graph to best fit line\n",
        "  sns.regplot(x=y_test_pred, y=y_test, line_kws={'color':'red'})\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('Actual')\n",
        "\n",
        "  # Evaluation metrics on train data\n",
        "  train_MSE  = round(mean_squared_error(y_train, y_train_pred),3)\n",
        "  train_RMSE = round(np.sqrt(train_MSE),3)\n",
        "  train_r2 = round(r2_score(y_train, y_train_pred),3)\n",
        "  train_MAE = round(mean_absolute_error(y_train, y_train_pred),3)\n",
        "  train_adj_r2 = round(1-(1-r2_score(y_train, y_train_pred))*((x_train.shape[0]-1)/(x_train.shape[0]-x_train.shape[1]-1)),3)\n",
        "  print(f'train MSE : {train_MSE}')\n",
        "  print(f'train RMSE : {train_RMSE}')\n",
        "  print(f'train MAE : {train_MAE}')\n",
        "  print(f'train R2 : {train_r2}')\n",
        "  print(f'train Adj R2 : {train_adj_r2}')\n",
        "  print('-'*120)\n",
        "\n",
        "  # Evaluation metrics on test data\n",
        "  test_MSE  = round(mean_squared_error(y_test, y_test_pred),3)\n",
        "  test_RMSE = round(np.sqrt(test_MSE),3)\n",
        "  test_r2 = round(r2_score(y_test, y_test_pred),3)\n",
        "  test_MAE = round(mean_absolute_error(y_test, y_test_pred),3)\n",
        "  test_adj_r2 = round(1-(1-r2_score(y_test, y_test_pred))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),3)\n",
        "  print(f'test MSE : {test_MSE}')\n",
        "  print(f'test RMSE : {test_RMSE}')\n",
        "  print(f'test MAE : {test_MAE}')\n",
        "  print(f'test R2 : {test_r2}')\n",
        "  print(f'test Adj R2 : {test_adj_r2}')\n",
        "  print('-'*100)\n",
        "\n",
        "# graph --> actual vs predicted on test data\n",
        "  plt.figure(figsize=(6,5))\n",
        "  plt.plot((y_test_pred)[:20])\n",
        "  plt.plot(np.array((y_test)[:20]))\n",
        "  plt.legend([\"Predicted\",\"Actual\"])\n",
        "  plt.xlabel('Test Data on last 20 points')\n",
        "  plt.show()\n",
        "  print('-'*100)\n",
        "\n",
        "  '''actual vs predicted value on test data'''\n",
        "  d = {'y_actual':y_test, 'y_predict':y_test_pred, 'error':y_test-y_test_pred}\n",
        "  print(pd.DataFrame(data=d).head().T)\n",
        "  print('-'*100)\n",
        "\n",
        "  # using the score from the performance metrics to create the final model_result.\n",
        "  result.append({'model':model_name,\n",
        "                  'train MSE':train_MSE,\n",
        "                  'test MSE':test_MSE,\n",
        "                  'train RMSE':train_RMSE,\n",
        "                  'test RMSE':test_RMSE,\n",
        "                  'train MAE':train_MAE,\n",
        "                  'test MAE':test_MAE,\n",
        "                  'train R2':train_r2,\n",
        "                  'test R2':test_r2,\n",
        "                  'train Adj R2':train_adj_r2,\n",
        "                  'test Adj R2':test_adj_r2})"
      ],
      "metadata": {
        "id": "XCrK2F2QI9ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 - Linear Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "predict(LinearRegression(), 'LinearRegression')\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 - Lasso (L1 Regularization)"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "\n",
        "*   Lasso (Least Absolute Shrinkage and Selection Operator) is a regularization technique used in linear regression models. It helps to reduce the complexity of the model and improve its generalization ability by penalizing the magnitude of coefficients of the features.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "predict(Lasso(alpha=0.1, max_iter=10000), 'Lasso')"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 - Ridge (L2 Regularization)\n",
        "\n",
        "*   Ridge Regression is a type of regularized linear regression that aims to solve the problem of multicollinearity and overfitting by adding a penalty term to the loss function. The penalty term is the L2 regularization term (also known as the weight decay term), which adds a penalty proportional to the square of the magnitude of the coefficients.\n"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "predict(Ridge(alpha=0.1, max_iter=10000), 'Ridge')\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4 - Elastic Net"
      ],
      "metadata": {
        "id": "v59hn_60zHEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   ElasticNet is a linear regression algorithm that combines both L1 (Lasso) and L2 (Ridge) regularization techniques. L1 and L2 regularization are methods used to prevent overfitting by adding penalty terms to the loss function that the algorithm minimizes. Lasso adds a penalty proportional to the absolute value of the coefficients, while Ridge adds a penalty proportional to the square of the coefficients.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qyVEwno5zWxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the elastic net model\n",
        "predict(ElasticNet(alpha=0.1, max_iter=10000), 'Elastic Net')"
      ],
      "metadata": {
        "id": "kcDoKousze8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5 - Decision Tree\n",
        "\n",
        "*   A decision tree is a tree-like model used in machine learning to make predictions or decisions by breaking down a set of rules or conditions into smaller and smaller sub-conditions, based on the values of the input features.\n",
        "\n",
        "*   Each node in the tree represents a test on a feature, and each branch represents the outcome of the test. The final branches of the tree, called the leaves, represent the class predictions or decisions. The tree is built recursively by finding the best feature to split the data based on the information gain or decrease in impurity at each node.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5J14XJj55qm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting and Visualizing the model\n",
        "predict(DecisionTreeRegressor(min_samples_leaf=20, min_samples_split=3,max_depth=20, random_state=2), 'Decision Tree')"
      ],
      "metadata": {
        "id": "GFjXCJos59sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 6 - Random Forest\n",
        "\n",
        "*   Random Forest is a machine learning algorithm that is used for both classification and regression tasks. It is a type of ensemble learning method, which combines multiple decision trees to form a more robust and accurate prediction model.\n",
        "\n",
        "\n",
        "*   In a Random Forest model, several decision trees are trained independently on random subsets of the training data, and their outputs are combined to produce a final prediction. The randomness in the algorithm comes from both the subset of training data used to train each tree and the features selected for each split in each tree.\n",
        "\n"
      ],
      "metadata": {
        "id": "l1iiaFTX6thl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter Tunning using GridSearchCV"
      ],
      "metadata": {
        "id": "yhXONHpb7L65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'n_estimators': [50,80],       # number of trees in the ensemble\n",
        "             'max_depth': [15,20],           # maximum number of levels allowed in each tree.\n",
        "             'min_samples_split': [5,15],    # minimum number of samples necessary in a node to cause node splitting.\n",
        "             'min_samples_leaf': [3,5]}      # minimum number of samples which can be stored in a tree leaf.\n",
        "          \n",
        "# Visualize RandomForestRegressor model\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Use GridSearchCV to perform a grid search over the parameter grid\n",
        "grid_search = GridSearchCV(rf, param_grid=param_grid, cv=5, scoring='r2')\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(x, y)"
      ],
      "metadata": {
        "id": "Y1F_2ErQ7RsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best parameters from the grid search\n",
        "rf_optimal_model = grid_search.best_estimator_\n",
        "rf_optimal_model"
      ],
      "metadata": {
        "id": "VoyB_g9P7UM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(rf_optimal_model, 'Random Forest')"
      ],
      "metadata": {
        "id": "snhGDwru8MS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Explainability"
      ],
      "metadata": {
        "id": "H7nJgGVk8mbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature importance \n",
        "importances = rf_optimal_model.feature_importances_\n",
        "\n",
        "# Creating a dictonary \n",
        "importance_dict = {'Feature' : list(x.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "# Creating the dataframe\n",
        "importance = pd.DataFrame(importance_dict)\n",
        "sorting_features = importance.sort_values(by=['Feature Importance'],ascending=False)\n",
        "sorting_features"
      ],
      "metadata": {
        "id": "WJ4LlMpM8sCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting feature importance graph\n",
        "plt.figure(figsize=(15,5))\n",
        "bar = sns.barplot(x='Feature Importance', y='Feature', data=sorting_features, color='blue')\n",
        "bar.set_title('Important Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fi1KJuQ88z4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   As per above graph it it very clear that important features are temp, hour, functioning day, humidity, solar radiation and rainfall\n"
      ],
      "metadata": {
        "id": "4UUjLK8n84Sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 7 - AdaBoost\n",
        "\n",
        "*   AdaBoost works by combining multiple \"weak\" classifiers into a single \"strong\" classifier. The algorithm begins by training a base classifier on the original training data. It then iteratively trains new classifiers on the same data, but with a weighted focus on instances that were misclassified by the previous classifiers. The final model is a weighted sum of all the classifiers, where the weights are determined by the accuracy of each classifier.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zmPIQMNf98sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a base decision tree regression model\n",
        "dt = DecisionTreeRegressor(max_depth=12)\n",
        "\n",
        "# Initialize the AdaBoost regression model\n",
        "ada = AdaBoostRegressor(base_estimator=dt, n_estimators=60, learning_rate=1, random_state =33)\n",
        "\n",
        "# Predict using function\n",
        "predict(ada, 'AdaBoost')"
      ],
      "metadata": {
        "id": "PV4UffHz-Sik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 8 - Xtreme Gradient Boosting\n",
        "\n",
        "*   Xtreme Gradient Boosting (XGBoost) is a popular machine learning algorithm for regression, classification, and ranking tasks. It is an extension of the gradient boosting method that uses a combination of gradient boosting and decision tree algorithms to produce highly accurate models.\n",
        "\n",
        "*   XGBoost uses a \"boosting\" technique to combine multiple weak predictive models into a strong one. The algorithm works by sequentially adding decision trees to the model, where each subsequent tree tries to correct the errors made by the previous trees. The final model is a weighted average of all the trees, where the weights are determined by the accuracy of each tree.\n",
        "\n"
      ],
      "metadata": {
        "id": "_eSZ9tXg8cWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'n_estimators': [300,500],     # number of trees in the ensemble\n",
        "             'max_depth': [7,8],             # maximum number of levels allowed in each tree.\n",
        "             'min_samples_split': [3,5],     # minimum number of samples necessary in a node to cause node splitting.\n",
        "             'min_samples_leaf': [3,5]}      # minimum number of samples which can be stored in a tree leaf.\n",
        "\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "xgb = XGBRegressor()\n",
        "\n",
        "# Use GridSearchCV to perform a grid search over the parameter grid\n",
        "grid_search = GridSearchCV(xgb, param_grid=param_grid, cv=5, scoring='r2')\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(x, y)"
      ],
      "metadata": {
        "id": "xdmKjusD9meL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best parameters from the grid search\n",
        "xgb_optimal_model = grid_search.best_estimator_\n",
        "xgb_optimal_model"
      ],
      "metadata": {
        "id": "jZAL5lwaDmSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(xgb_optimal_model, 'XGB')"
      ],
      "metadata": {
        "id": "_U-oT-PTDnud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Explainability"
      ],
      "metadata": {
        "id": "OsfdMzknDt0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature importance \n",
        "importances = xgb_optimal_model.feature_importances_\n",
        "\n",
        "# Creating a dictonary \n",
        "importance_dict = {'Feature' : list(x.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "# Creating the dataframe\n",
        "importance = pd.DataFrame(importance_dict)\n",
        "sorting_features = importance.sort_values(by=['Feature Importance'],ascending=False)\n",
        "sorting_features"
      ],
      "metadata": {
        "id": "msOuDY8bDxsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting feature importance graph\n",
        "plt.figure(figsize=(15,5))\n",
        "bar = sns.barplot(x='Feature Importance', y='Feature', data=sorting_features, color='blue')\n",
        "bar.set_title('Important Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o0rbjjJZD7Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   As per above graph it it very clear that important features are functioning day, season winter and rainfall\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2ZwBL8BtEC6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML Model - 9 - Light GBM\n",
        "\n",
        "*   Light GBM is similar to other gradient boosting frameworks such as XGBoost and CatBoost, but it differs in how it grows trees. Unlike other frameworks that grow trees leaf-wise, Light GBM grows trees in a depth-wise manner, which makes it much faster and more memory-efficient. Light GBM also uses a histogram-based approach to find the best split points, which further improves its efficiency."
      ],
      "metadata": {
        "id": "i7tRhVYxBC45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'n_estimators': [600,800],     # number of trees in the ensemble\n",
        "             'max_depth': [8,10],            # maximum number of levels allowed in each tree.\n",
        "             'min_samples_split': [3,5],     # minimum number of samples necessary in a node to cause node splitting.\n",
        "             'min_samples_leaf': [2,3]}      # minimum number of samples which can be stored in a tree leaf."
      ],
      "metadata": {
        "id": "dGB8K38w_c-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the RandomForestRegressor model\n",
        "lgb = LGBMRegressor()"
      ],
      "metadata": {
        "id": "9tsE-vfH_0ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GridSearchCV to perform a grid search over the parameter grid\n",
        "grid_search = GridSearchCV(lgb, param_grid=param_grid, cv=5, scoring='r2')"
      ],
      "metadata": {
        "id": "Jj7s47w6_2xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the training data\n",
        "grid_search.fit(x, y)"
      ],
      "metadata": {
        "id": "2PETEcgi_5gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best parameters from the grid search\n",
        "lgb_optimal_model = grid_search.best_estimator_\n",
        "lgb_optimal_model"
      ],
      "metadata": {
        "id": "vDLLH9OhAWtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(lgb_optimal_model, 'LGB')"
      ],
      "metadata": {
        "id": "5wxOWAfuAeJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Explainability "
      ],
      "metadata": {
        "id": "f5NOqhnnBfOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature importance \n",
        "importances = lgb_optimal_model.feature_importances_\n",
        "\n",
        "#Creating a dictonary \n",
        "importance_dict = {'Feature' : list(x.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "#Creating the dataframe\n",
        "importance = pd.DataFrame(importance_dict)\n",
        "sorting_features = importance.sort_values(by=['Feature Importance'],ascending=False)\n",
        "sorting_features"
      ],
      "metadata": {
        "id": "RLlHm8cmAmNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting feature importance graph\n",
        "plt.figure(figsize=(15,5))\n",
        "bar=sns.barplot(x='Feature Importance', y='Feature', data=sorting_features, color='blue')\n",
        "bar.set_title('Important Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "te4xNjZYAqGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   As per above graph it it very clear that important features are temperature, humidity, hour, visibility, day, windspeed, solar radiation, and month"
      ],
      "metadata": {
        "id": "D_2ZO0baEPai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.0 Model Result "
      ],
      "metadata": {
        "id": "xt3xJM4MEldQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the model_result list into DataFrame\n",
        "result = pd.DataFrame(result)"
      ],
      "metadata": {
        "id": "u8bDHTTpEpIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sorting the values by test R2 score\n",
        "result.sort_values(by='test R2', ascending=False)"
      ],
      "metadata": {
        "id": "YLcB9XOWE7P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting graph to compare model performance of all the models\n",
        "fig, ax = plt.subplots(1,2, figsize=(16,6))\n",
        "sns.barplot(x=result['model'], y=result['test R2'], ax=ax[0])           # Model Vs test R2\n",
        "sns.barplot(x=result['model'], y=result['test Adj R2'], ax=ax[1])       # Model Vs test Adj R2\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "unNEc6-kFERj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   The fit model to the dependent variables can be evaluated using R-Square measure"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   From above graph we can select LGB and XGB Regressor.\n",
        "2.   We will select LGB finally, Because LGB have lower RMSE value and highest R2 score on test data\n",
        "\n",
        "3.   Also LGB is better suitable for larger dataset\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **In this project, we looked into a number of variables that affected the number of bicycle rentals. An overview of exploratory data analysis is provided below.**\n",
        "\n",
        "1.   **It is orbserved that two pattern for Working and Non-working day. the first is for working days, where the rental count is highest between the hours of 8 a.m. and 5 p.m., and the second is for non-working days, where the rental count is more or less consistent throughout the day with a peak at around midday.**\n",
        "2.   **Hour of the day: The number of bike rentals is largely connected with the time of day. As previously stated, the count peaks at peak hours on a working day and is mostly uniform during the day on a non-working day**\n",
        "\n",
        "3.   **Temperature: Most people love biking in temperatures ranging from moderate to high. In the range of 32 to 36 degrees Celsius, we see the highest rental rates.**\n",
        "4.   **Season: We see highest number bike rentals in Fall (July to September) and Summer (April to June)\n",
        "Seasons and the lowest in Spring (January to March) season**\n",
        "\n",
        "5.   **Weather: As one might anticipate, we have the largest number of bike rentals on clear days and the lowest on days that are snowy or rainy.**\n",
        "6.   **Humidity: We see a reduction in the number of bike rentals as the humidity rises.**\n",
        "\n",
        "\n",
        "\n",
        "7.   **The majority of bike is rented by the working professionals on office time and students at college time**\n",
        "8.   **If business team planning for extra bikes and station peak timing should be consider**\n",
        "\n",
        "9.   **And for maintainance and servicing should be done at night due low demand of bike at that time**\n",
        "10.   **Above all models, I have chosen the Light GBM model. Time isn't a compelling factor in this situation; I want greater expectations for the rented_bike_count. Because of this, accuracy was increased by using several linear models, decision trees, Random Forests, and Gradient Boost approaches. To select a model, I compared R2 metrics. And LGB have lower RMSE value and highest R2 score on test data**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}